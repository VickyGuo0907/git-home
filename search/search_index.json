{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Vicky Guo's GitHub Home","text":""},{"location":"#about-me","title":"About Me","text":"<p>I am a Mission-Driven and passionate developer, deep-thinker, and a lifelong learner, who focuses on the outcomes, enjoys researching new technologies and coding approaches to solve real-life problems through technologies.\u00a0</p> <p>Senior Software Developer with 14+ years\u2019 experience delivering enterprise SaaS, Cloud and AI solutions. Leader of couple projects plan and delivery, and leading Inner source culture transformation in Organization. Strong practitioner of Agile methodologies and certified Scrum Master. Strong attention to details, ability to take ownership over specific projects and workstream. Experienced with all phases of development from Architecture to DevOps. Possesses a continuous improvement mindset, focused on improving development process and outcomes. A continuous learner who is passionate about mastering new technologies to deliver innovative solutions that solve real-world business problems. Consistent grow and mentor other developers and team members, also passionately sharing knowledge and experiences.</p>"},{"location":"#next-career-path","title":"Next Career Path","text":"<p>Since ML/AI and Big Data - is a quick-growing area now, but AI algorithm, the data scientist isn't the only part of it, Software Engineer/ Machine learning Engineer would be essential role to deliver AI solutions. I would want to pursue this path and contribute to the data pipeline to deliver AI products. </p> <p>I believe that my limitless energy and enthusiasm, my passion for outstanding development work, and learning new skills consistently would allow me to work as a strong contributing member in my future Role.</p>"},{"location":"#contact-me","title":"Contact Me","text":"<ul> <li>Email: vicky.guo97@gmail.com</li> <li>Linkedin</li> <li>Medium</li> </ul>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2021/03/07/docker-commands/","title":"Docker Commands","text":"<p>Docker helps developers build, share, run, and verify applications anywhere \u2014 without tedious environment configuration or management. Docker link</p> <p>Below list most useful Docker Commands for reference.  </p>"},{"location":"blog/2021/03/07/docker-commands/#essential-toolkit-for-docker","title":"Essential Toolkit for docker","text":"<p>Details check docker docs</p>"},{"location":"blog/2021/03/07/docker-commands/#1-version","title":"1. version","text":"<pre><code>docker --version\n</code></pre>"},{"location":"blog/2021/03/07/docker-commands/#2-download-images","title":"2. download images","text":"<p>Pull an image or a repository from a registry.  docker pull [OPTIONS] NAME[:TAG|@DIGEST]</p> <pre><code># pull from docker hub\ndocker pull ubuntu:14.04\n\n# Pull from a different registry\ndocker pull myregistry.local:5000/testing/test-image\n</code></pre>"},{"location":"blog/2021/03/07/docker-commands/#3-list-images","title":"3. list images","text":"<p>List all the docker images pulled on the system with image details </p> <pre><code>docker images\n</code></pre>"},{"location":"blog/2021/03/07/docker-commands/#4-build-image","title":"4. build image","text":"<p>Build an image from a Dockerfile. docker build [OPTIONS] PATH | URL | -</p> <pre><code># Build with PATH \ndocker build . \n\n# Tag an image with (-t) and Specify a Dockerfile with (-f)\ndocker build -f Dockerfile.debug -t vieux/apache:2.0 .\n</code></pre>"},{"location":"blog/2021/03/07/docker-commands/#5-run-image","title":"5. Run image","text":"<p>Run the docker image mentioned in the command.  docker run [OPTIONS] IMAGE[:TAG|@DIGEST] [COMMAND] [ARG...]</p> <pre><code># run detached \ndocker run -d -p 80:80 my_image \n</code></pre>"},{"location":"blog/2021/03/07/docker-commands/#6-list-containers","title":"6. List containers","text":"<p>lists all the docker containers are running with container details.</p> <pre><code>docker ps\n\n# List all the docker containers running/exited/stopped with container details.\ndocker ps -a\n</code></pre>"},{"location":"blog/2021/03/07/docker-commands/#7-access-container","title":"7. Access container","text":"<p>Access the docker container and run commands inside the container.</p> <pre><code>docker exec -it my_image bash\n</code></pre>"},{"location":"blog/2021/03/07/docker-commands/#8-removing-container","title":"8. Removing container","text":"<p>Remove the docker container with container id mentioned in the command.</p> <pre><code>docker rm my-container\n</code></pre>"},{"location":"blog/2021/03/07/docker-commands/#8-removing-image","title":"8. Removing image","text":"<p>Remove the docker image with the docker image id mentioned in the command.</p> <pre><code>docker rmi my-image\n</code></pre>"},{"location":"blog/2021/03/07/docker-commands/#9-start-docker-stop-docker","title":"9. start Docker || Stop Docker","text":"<p>Start or Stop the docker container with container id mentioned in the command.</p> <pre><code>docker start my-container\ndocker stop my-container\n\ndocker restart my-container\n</code></pre>"},{"location":"blog/2021/03/07/docker-commands/#docker-cheat-sheet","title":"docker cheat sheet","text":"<p>PDF link</p>"},{"location":"blog/2021/03/09/helm-chart-commands/","title":"Helm chart commands","text":"<p>Helm helps you manage Kubernetes applications \u2014 Helm Charts help you define, install, and upgrade even the most complex Kubernetes application. Helm link</p> <p>Below list most useful Helm Commands for reference.  </p>"},{"location":"blog/2021/03/09/helm-chart-commands/#essential-toolkit-for-helm","title":"Essential Toolkit for helm","text":""},{"location":"blog/2021/03/09/helm-chart-commands/#1-version","title":"1. version","text":"<pre><code>helm version\n</code></pre>"},{"location":"blog/2021/03/09/helm-chart-commands/#2-list-installed-releases","title":"2. List installed releases","text":"<pre><code>helm list\nhelm get values &lt;release&gt;\n</code></pre>"},{"location":"blog/2021/03/09/helm-chart-commands/#3-install-package","title":"3. install package","text":"<pre><code>helm install -f ./override.yaml &lt;name&gt; &lt;chart&gt; [--namespace &lt;ns&gt;]\n\nhelm install mychart-0.1.0.tgz --dry-run --debug       # Test installing\n</code></pre>"},{"location":"blog/2021/03/09/helm-chart-commands/#4-upgrading-releases","title":"4. upgrading releases","text":"<pre><code>helm upgrade &lt;name&gt; [--namespace &lt;ns&gt;]\nhelm upgrade --wait &lt;name&gt;   # Wait for pods to come up\n</code></pre>"},{"location":"blog/2021/03/09/helm-chart-commands/#5-delete-release","title":"5. delete release","text":"<pre><code>helm delete --purge &lt;name&gt;\n</code></pre>"},{"location":"blog/2021/03/09/helm-chart-commands/#6-get-release","title":"6. get release","text":"<pre><code>helm get &lt;deployment-name&gt;\n</code></pre>"},{"location":"blog/2021/03/09/helm-chart-commands/#7-helm-lint","title":"7. helm lint","text":"<pre><code>helm lint &lt;chart-name&gt;\n</code></pre>"},{"location":"blog/2021/03/08/kubernetes-kubectl-commands/","title":"Kubernetes kubectl commands","text":"<p>Kubernetes provides a command line tool for communicating with a Kubernetes cluster's control plane, using the Kubernetes API.</p> <p>This tool is named kubectl. kubectl link</p> <p>Below list most useful Helm Commands for reference.  </p>"},{"location":"blog/2021/03/08/kubernetes-kubectl-commands/#1-kubectl-get","title":"1. kubectl get","text":"<p>Use get to pull a list of resources you have currently on your cluster. The types of resources you can get include:</p> <ul> <li>Namespace</li> <li>Pod</li> <li>Node</li> <li>Deployment</li> <li>Service</li> <li>ReplicaSets</li> </ul> <pre><code>kubectl get ns\nkubectl get pods -n kube-system\nkubectl get nodes\nkubectl get services\n</code></pre>"},{"location":"blog/2021/03/08/kubernetes-kubectl-commands/#2-kubectl-describe","title":"2. kubectl describe","text":"<p>Describe shows the details of the resource you're looking at.</p> <p>Resources you can describe include:</p> <ul> <li>Nodes</li> <li>Pods</li> <li>Services</li> <li>Deployments</li> <li>Replica sets</li> </ul> <pre><code>kubectl describe pods my-pods -n kube-system\n\nkubectl describe servcies my-services -n kube-system\n</code></pre>"},{"location":"blog/2021/03/08/kubernetes-kubectl-commands/#3-kubectl-logs","title":"3. kubectl logs","text":"<p>logs offer detailed insights into what's happening inside Kubernetes in relation to the pod.</p> <pre><code>kubectl logs my-pod -n kube-system\n</code></pre>"},{"location":"blog/2021/03/08/kubernetes-kubectl-commands/#4-kubectl-exec","title":"4. kubectl exec","text":"<p>exec into a container to troubleshoot an application directly.</p> <pre><code>kubectl exec -it my-pod -n kube-system /bin/bash\n</code></pre>"},{"location":"blog/2021/03/08/kubernetes-kubectl-commands/#5-kubectl-cp","title":"5. kubectl cp","text":"<p>This command is for copying files and directories to and from containers, much like the Linux cp command. The syntax follows a kubectl cp   format: <pre><code>kubectl cp commands_copy.txt charts/cherry-chart-88d49478c-dmcfv:commands.txt\n</code></pre>"},{"location":"blog/2021/03/08/kubernetes-kubectl-commands/#kubectl-cheat-sheet","title":"kubectl cheat sheet","text":"<p>PDF link </p>"},{"location":"blog/2020/03/07/linux-shell-script/","title":"Linux Shell Script","text":"<p>Below list most useful Linux Shell Script for reference.  </p>"},{"location":"blog/2020/03/07/linux-shell-script/#essential-toolkit","title":"Essential Toolkit","text":""},{"location":"blog/2020/03/07/linux-shell-script/#1-alias","title":"1. alias","text":"<p>The alias command lets you give your own name to a command or sequence of commands. You can then type your short name, and the shell will execute the command or sequence of commands for you.</p> <pre><code>alias cls=clear\n</code></pre> <pre><code># create alias allow to find process ID (PID)\n# then pipes them through grep command\nalias pf=\"ps -e | grep $1\"\n</code></pre>"},{"location":"blog/2020/03/07/linux-shell-script/#2-cat","title":"2. cat","text":"<p>The cat command (short for \u201cconcatenate\u201d) lists the contents of files to the terminal window.</p> <pre><code>cat .bash_logout\n</code></pre> <p>With files longer than the number of lines in your terminal window, the text will whip past too fast for you to read. You can pipe the output from cat through less to make the process more manageable.  With less you can scroll forward and backward through the file using the Up and Down Arrow keys, the PgUp and PgDn keys, and the Home and End keys. Type q to quit from less.</p> <pre><code>cat .bashrc | less\n</code></pre>"},{"location":"blog/2020/03/07/linux-shell-script/#3-chmod","title":"3. chmod","text":"<p>The chmod command sets the file permissions flags on a file or folder. The flags define who can read, write to or execute the file. </p> <p><pre><code>#-owner|group|others\n-rwxrwxrwx\n</code></pre> If the first character is a - the item is a file, if it is a d the item is a directory. The rest of the string is three sets of three characters. From the left, the first three represent the file permissions of the owner, the middle three represent the file permissions of the group and the rightmost three characters represent the permissions for others. In each set, an r stands for read, a w stands for write, and an x stands for execute.</p> <p>One way to use chmod is to provide the permissions you wish to give to the owner, group, and others as a 3 digit number.  The leftmost digit represents the owner. The middle digit represents the group. The rightmost digit represents the others. The digits you can use and what they represent are listed here:</p> <pre><code>0: No permission\n1: Execute permission\n2: Write permission\n3: Write and execute permissions\n4: Read permission\n5: Read and execute permissions\n6: Read and write permissions\n7: Read, write and execute permissions\n\nchmod -R 765 example.txt\n</code></pre>"},{"location":"blog/2020/03/07/linux-shell-script/#4-chown","title":"4. chown","text":"<p>The chown command allows you to change the owner and group owner of a file. You can use chown to change the owner or group, or both of a file. You must provide the name of the owner and the group, separated by a : character. You will need to use sudo. To retain dave as the owner of the file but to set mary as the group owner, use this command:</p> <pre><code>sudo chown dave:mary example.txt\n</code></pre>"},{"location":"blog/2020/03/07/linux-shell-script/#5-curl","title":"5. curl","text":"<p>The curl command is a tool to retrieve information and files from Uniform Resource Locators (URLs) or internet addresses.</p> <p>The curl command may not be provided as a standard part of your Linux distribution. Use apt-get to install this package onto your system if you\u2019re using Ubuntu or another Debian-based distribution. On other Linux distributions, use your Linux distribution\u2019s package management tool instead.</p> <pre><code>sudo apt-get install curl\n</code></pre> <p>This command retrieves the file for us. Note that you need to specify the name of the file to save it in, using the -o (output) option. If you do not do this, the contents of the file are scrolled rapidly in the terminal window but not saved to your computer.</p> <pre><code>curl https://raw.githubusercontent.com/torvalds/linux/master/kernel/events/core.c -o core.c\n</code></pre> <p>test web site request in container</p> <pre><code>curl https://google.com\n</code></pre>"},{"location":"blog/2020/03/07/linux-shell-script/#6-df","title":"6. df","text":"<p>The df command shows the size, used space, and available space on the mounted filesystems of your computer.</p> <p>Two of the most useful options are the -h (human readable) and -x (exclude) options. The human-readable option displays the sizes in Mb or Gb instead of in bytes.</p> <pre><code>df -h -x squashfs\n</code></pre>"},{"location":"blog/2020/03/07/linux-shell-script/#7-diff","title":"7. diff","text":"<p>The diff command compares two text files and shows the differences between them. There are many options to tailor the display to your requirements.</p> <p>The -y (side by side) option shows the line differences side by side. The -w (width) option lets you specify the maximum line width to use to avoid wraparound lines. The two files are called alpha1.txt and alpha2.txt in this example. The --suppress-common-lines prevents diff from listing the matching lines, letting you focus on the lines which have differences.</p> <pre><code>diff -y -W 70 alpha1.txt alpha2.txt --suppress-common-lines\n</code></pre>"},{"location":"blog/2020/03/07/linux-shell-script/#8-echo","title":"8. echo","text":"<p>The echo command prints (echoes) a string of text to the terminal window.</p> <pre><code>echo $USER\necho $HOME\necho $PATH\n</code></pre>"},{"location":"blog/2020/03/07/linux-shell-script/#9-find","title":"9. find","text":"<p>Use the find command to track down files that you know exist if you can\u2019t remember where you put them.</p> <p><pre><code>find . -name *ones*\n</code></pre> We do this using the -type option with the f parameter. The f parameter stands for files.</p> <pre><code>find . -type f -name *ones*\n</code></pre> <p>If you want the search to be case insensitive use the -iname (insensitive name) option.</p> <pre><code>find . -iname *wild*\n</code></pre>"},{"location":"blog/2020/03/07/linux-shell-script/#10-finger","title":"10. finger","text":"<p>The finger command gives you a short dump of information about a user, including the time of the user\u2019s last login, the user\u2019s home directory, and the user account\u2019s full name.</p> <pre><code>finger $USER\n</code></pre>"},{"location":"blog/2020/03/07/linux-shell-script/#11-free","title":"11. free","text":"<p>The free command gives you a summary of the memory usage with your computer. It does this for both the main Random Access Memory (RAM) and swap memory. The -h (human) option is used to provide human-friendly numbers and units. Without this option, the figures are presented in bytes.</p> <pre><code>free -h\n</code></pre>"},{"location":"blog/2020/03/07/linux-shell-script/#12-grep","title":"12. grep","text":"<p>The grep utility searches for lines which contain a search pattern. When we looked at the alias command, we used grep to search through the output of another program, ps . The grep command can also search the contents of files. Here we\u2019re searching for the word \u201ctrain\u201d in all text files in the current directory.</p> <pre><code>grep train *.txt\n</code></pre>"},{"location":"blog/2020/03/07/linux-shell-script/#13-groups","title":"13. groups","text":"<p>The groups command tells you which groups a user is a member of.</p> <pre><code>groups mary\n</code></pre>"},{"location":"blog/2020/03/07/linux-shell-script/#14-gzip","title":"14. gzip","text":"<p>The gzip command compresses files. By default, it removes the original file and leaves you with the compressed version. To retain both the original and the compressed version, use the -k (keep) option.</p> <pre><code>gzip -k core.c\n</code></pre>"},{"location":"blog/2020/03/07/linux-shell-script/#15-head","title":"15. head","text":"<p>The head command gives you a listing of the first 10 lines of a file. If you want to see fewer or more lines, use the -n (number) option. In this example, we use head with its default of 10 lines. We then repeat the command asking for only five lines.</p> <pre><code>head -core.c\nhead -n 5 core.c\n</code></pre>"},{"location":"blog/2020/03/07/linux-shell-script/#16-history","title":"16. history","text":"<p>The history command lists the commands you have previously issued on the command line. You can repeat any of the commands from your history by typing an exclamation point ! and the number of the command from the history list.</p> <pre><code>history\n\n# pick the number\n!199\n</code></pre>"},{"location":"blog/2020/03/07/linux-shell-script/#17-kill","title":"17. kill","text":"<p>The kill command allows you to terminate a process from the command line. You do this by providing the process ID (PID) of the process to kill. Don\u2019t kill processes willy-nilly. You need to have a good reason to do so. In this example, we\u2019ll pretend the shutter program has locked up.</p> <pre><code># Find PID first\nps -e | grep shutter.\n\nkill 1692\n</code></pre>"},{"location":"blog/2020/03/07/linux-shell-script/#18-less","title":"18. less","text":"<p>The less command allows you to view files without opening an editor. It\u2019s faster to use, and there\u2019s no chance of you inadvertently modifying the file. With less you can scroll forward and backward through the file using the Up and Down Arrow keys, the PgUp and PgDn keys and the Home and End keys. Press the Q key to quit from less.</p> <pre><code>less core.c\n</code></pre> <p>You can also pipe the output from other commands into less. To see the output from ls for a listing of your entire hard drive, use the following command:</p> <pre><code>ls -R / | less\n</code></pre>"},{"location":"blog/2020/03/07/linux-shell-script/#19-ls","title":"19. ls","text":"<p>more details check man page</p> <pre><code># use the -l (long) option\nls -l\n\n# To use human-friendly file sizes include the -h (human) option\nls -lh\n\n# To include hidden files use the -a (all files) option\nls -lha\n</code></pre>"},{"location":"blog/2020/03/07/linux-shell-script/#20-man","title":"20. man","text":"<p>The man command displays the \u201cman pages\u201d for a command in less . The man pages are the user manual for that command. Because man uses less to display the man pages, you can use the search capabilities of less.</p> <p>For example, to see the man pages for chown, use the following command:</p> <pre><code>man chown\n</code></pre>"},{"location":"blog/2020/03/07/linux-shell-script/#21-tree","title":"21. tree","text":"<p>tree is a recursive directory listing program that produces a depth-indented listing of files.</p> <pre><code># If you are using Apple OS X, type:\nbrew install tree\n\n# Display the tree hierarchy of a directory\ntree -a ./GFG\n\n## list files with their permissions\ntree -p ./GFG \n</code></pre>"},{"location":"blog/2020/03/07/linux-shell-script/#22-mkdir","title":"22. mkdir","text":"<p>The mkdir command allows you to create new directories in the filesystem. </p> <pre><code>mkdir invoces\n\nmkdir -p quotes/yearly/2019\n</code></pre>"},{"location":"blog/2020/03/07/linux-shell-script/#23-mv","title":"23. mv","text":"<p>The mv command allows you to move files and directories from directory to directory. It also allows you to rename files.</p> <pre><code>mv ~/Documents/Ukulele/Apache.pdf .\n\n# rename file\nmv Apache.pdf The_Shadows_Apache.pdf\n</code></pre>"},{"location":"blog/2020/03/07/linux-shell-script/#24-passwd","title":"24. passwd","text":"<p>The passwd command lets you change the password for a user. Just type passwd to change your own password.</p> <pre><code>sudo passwd mary\n</code></pre>"},{"location":"blog/2020/03/07/linux-shell-script/#25-ping","title":"25. ping","text":"<p>The ping command lets you verify that you have network connectivity with another network device.</p> <pre><code>ping 192.168.4.18\n\n# ping to run for a specific number of ping attempts, use the -c (count) option\nping -c 5 192.168.4.18\n\n# To hear a ping use the -a(audible) option.\nping -a 192.168.4.18\n</code></pre>"},{"location":"blog/2020/03/07/linux-shell-script/#26-ps","title":"26. ps","text":"<p>The ps command lists running processes. Using ps without any options causes it to list the processes running in the current shell.</p> <pre><code>ps\n\n# to see all the processes related to a particular user\nps -u dave | less\n\n# To see every process that is running, use the -e( every process)\nps -e | less\n\n#  Shows User, All Processes and Terminal Information\nps aux\n</code></pre>"},{"location":"blog/2020/03/07/linux-shell-script/#27-pwd","title":"27. pwd","text":"<p>Nice and simple, the pwd command prints the working directory (the current directory) from the root / directory.</p> <pre><code>pwd\n</code></pre>"},{"location":"blog/2020/03/07/linux-shell-script/#28-shutdown","title":"28. shutdown","text":"<p>The shutdown command lets you shut down or reboot your Linux system.</p> <pre><code>shutdown\n\n# shut down immediately\nshutdown now\n</code></pre>"},{"location":"blog/2020/03/07/linux-shell-script/#29-ssh","title":"29. SSH","text":"<p>Use the ssh command to make a connection to a remote Linux computer and log into your account.To make a connection, you must provide your user name and the IP address or domain name of the remote computer.</p> <pre><code>ssh mary@192.168.4.23\n</code></pre>"},{"location":"blog/2020/03/07/linux-shell-script/#30-sudo","title":"30. sudo","text":"<p>The sudo command is required when performing actions that require root or superuser permissions, such as changing the password for another user.</p> <pre><code>sudo passwd mary\n</code></pre>"},{"location":"blog/2020/03/07/linux-shell-script/#31-tail","title":"31. tail","text":"<p>The tail command gives you a listing of the last 10 lines of a file. If you want to see fewer or more lines, use the -n (number) option.we use tail with its default of 10 lines. We then repeat the command asking for only five lines.</p> <pre><code>tail -n 5 core.c\n</code></pre>"},{"location":"blog/2020/03/07/linux-shell-script/#32-tar","title":"32. tar","text":"<p>With the tar command, you can create an archive file (also called a tarball) that can contain many other files. * -c (create) option * -v (verbose) option * -f (filename) option * -z (gzip) option * -j(bzip2) option * -x (extract)</p> <pre><code>tar -cvf songs.tar Ukulele/\n\ntar -cvzf songs.tar.gz Ukulele/\n\ntar -cvjf songs.tar.bz2 Ukulele/\n\n# extract files\ntar -xvf songs.tar\n\ntar -xvzf songs.tar.gz\n\ntar -xvjf songs.tar.bz2\n</code></pre>"},{"location":"blog/2020/03/07/linux-shell-script/#33-whoami-or-w","title":"33. whoami or w","text":"<p>The w command lists the currently logged in users.</p> <pre><code>w\n</code></pre> <p>Use whoami to find out who you are logged in as or who is logged into an unmanned Linux terminal.</p> <pre><code>whoami\n</code></pre>"},{"location":"blog/2020/03/07/linux-shell-script/#34-top","title":"34. top","text":"<p>The top command shows you a real-time display of the data relating to your Linux machine. The top of the screen is a status summary. Detail check here</p> <pre><code>top\n</code></pre>"},{"location":"blog/2020/03/07/linux-shell-script/#35-uname","title":"35. uname","text":"<p>You can obtain some system information regarding the Linux computer you\u2019re working on with the uname command.</p> <pre><code># See everything\nuname -a \n\n# Use the -s (kernel name) option to see the type of kernel.\nuname -s \n\n# Use the -r (kernel release) option to see the kernel release.\nuname -r \n\n# Use the -v (kernel version) option to see the kernel version.\nuname -v \n</code></pre>"},{"location":"blog/2020/03/07/linux-shell-script/#36-hostname","title":"36. hostname","text":"<p>hostname used to either display or, with appropriate privileges, set the current host name of the system. The host name is used by many applications to identify the machine.</p> <pre><code>hostname -s \n</code></pre>"},{"location":"blog/2020/03/07/linux-shell-script/#37-du","title":"37. du","text":"<p>The Linux \u201cdu\u201d (Disk Usage) is a standard Unix/Linux command, used to check the information of disk usage of files and directories on a machine. </p> <pre><code># Using \u201c-h\u201d option with \u201cdu\u201d command provides results in \u201cHuman Readable Format\u201c\ndu -h /home/tecmint  \n\n# To get the summary of a grand total disk usage size of an directory use the option \u201c-s\u201d as follows.\ndu -sh /home/tecmint \n\n# Using \u201c-a\u201d flag with \u201cdu\u201d command displays the disk usage of all the files and directories.\ndu -a /home/tecmint \n</code></pre>"},{"location":"blog/2024/01/28/test-and-validate-data-pipelinesworkflows-with-prefect/","title":"Test and Validate data pipelines/workflows with Prefect","text":"<p>Using non standard test automation tool Prefect to Test and Validate your data pipelines.</p> <p>Published on Medium</p>"},{"location":"blog/2024/01/28/test-and-validate-data-pipelinesworkflows-with-prefect/#describing-the-problem-i-faced","title":"Describing the problem I faced","text":"<p>AI is presently a day-to-day hot topic in today\u2019s world. Naturally, data is a crucial component of AI models and products. For this reason, most companies create their own data pipeline/workflow to manage their data for when building their AI models or future data analysis.</p> <p>As we know, data pipelines can be complex and overwhelming, requiring significant time and effort to ensure the quality of all related services and data for the development team. The recent project I attended put me into that spot, I started asking myself some questions:</p> <p>When it comes to API-level testing and end-to-end integration testing for data-driven products, we always rely on mainstream automation testing platforms or tools, which require endless efforts to implement and maintain. However, is there a different solution that can give us the same outcome but enhance the efficiency and extensibility of validating data workflows/pipelines? Except for the main purpose of testing and validation, could we explore additional functionality through this solution? I always believe the combination of in-house innovation and open-source collaboration can be a powerful driver of progress and competitiveness for large corporations or any company. But How?</p>"},{"location":"blog/2024/01/28/test-and-validate-data-pipelinesworkflows-with-prefect/#the-desired-solution","title":"The Desired Solution","text":"<p>While the Modern Data Stack continuously grows and provides a lot of flexibility with the various components that play well with each other, open-source orchestration tools like Apache Airflow, Prefect, Argo, and Kubeflow give you more options to fit different purposes.</p> <p>After conducting some research and comparison, I have set my sights on Prefect platform. This article won\u2019t be an introduction to Prefect, You can get more details about Prefect from here.</p> <p></p>"},{"location":"blog/2024/01/28/test-and-validate-data-pipelinesworkflows-with-prefect/#here-are-some-reasons-why-i-chose-it","title":"Here are some reasons why I chose it:","text":"<ul> <li> <p>Prefect is a modern workflow orchestrator platform, that makes it easier to create, deploy, and monitor tasks. With the help of an orchestrator, teams can quickly build repeatable processes that are easy to run remotely, observe, and troubleshoot if they fail.</p> </li> <li> <p>Prefect would be suitable if you need to get something lightweight up and running as soon as possible.</p> </li> <li> <p>Prefect offers a comprehensive UI dashboard that provides all the necessary information related to flows and tasks, work pool details, and workflow logs.</p> </li> <li> <p>Additionally, Prefect offers both cloud-hosted and self-hosted deployment strategies, allowing users to maintain it using their own CI/CD pipeline, providing greater flexibility.</p> </li> <li> <p>Prefect integrations are structured as collections of pre-built tasks, flows, blocks, and more, which can be easily installed as PyPI packages. They offer many popular extensive capabilities.</p> </li> </ul>"},{"location":"blog/2024/01/28/test-and-validate-data-pipelinesworkflows-with-prefect/#solution-how-to-use-prefect-for-testing-and-data-pipeline-validation","title":"Solution: How to use Prefect for testing and data pipeline validation","text":"<p>Prefect have two key concepts Tasks and Flows you should know.</p> <ul> <li> <p>Tasks: Discrete units of work in a Prefect workflow.</p> </li> <li> <p>Flows: A Prefect workflow, defined as a Python function.</p> </li> </ul> <p>Here is my idea,</p> <ul> <li> <p>Define task units as each of my API entry points.</p> </li> <li> <p>Define flows to invoke different combinations of tasks to represent different data workflows.</p> </li> </ul> <p>This would easily include regular or customized data workflow, Additionally, this approach could streamline your workflow, enhance code readability and most importantly to focus on business requirements.</p>"},{"location":"blog/2024/01/28/test-and-validate-data-pipelinesworkflows-with-prefect/#sample-code-task","title":"Sample Code : Task","text":"<ul> <li>You can almost effortlessly include all REST API CRUD operations as tasks, such as GET, CREATE, UPDATE, DELETE API entry points.</li> </ul> <ul> <li>Let\u2019s check what a User GET entry point (task) would look like. It\u2019s just a normal python http request function with Prefect task respective decorator.</li> </ul> <pre><code>@task(name=\"get_user_task\", description=\"get user information\", log_prints=True)\ndef get_user_task(api_token: str, user_id: str) -&gt; dict:\n    \"\"\"\n    :param api_token:\n    :param user_id:\n    :return:\n    \"\"\"\n    logger = get_run_logger()\n    dict_result = {}\n    request_headers = {\"Authorization\": f\"Bearer {api_token}\",\n                       \"content-type\": \"application/json\"}\n    try:\n        response = requests.get(USER_API_URL, headers=request_headers, params={\"user_id\": user_id})\n        dict_result[\"status_code\"] = response.status_code\n        dict_result[\"result\"] = json.loads(response.content)\n        if response.status_code!=200:\n            logger.warning(\"failed to get user information, error: %s\", response.content)\n    except Exception as err:\n        dict_result[\"status_code\"] = 502\n        dict_result[\"result\"] = str(err)\n        logger.error(\"get_user_task request failed, error: %s\", err)\n\n    return dict_result\n</code></pre>"},{"location":"blog/2024/01/28/test-and-validate-data-pipelinesworkflows-with-prefect/#sample-code-flow","title":"Sample Code : Flow","text":"<ul> <li>Let start by defining a Prefect flow now. below is one simple flow which will get a existing user -&gt; create a new user -&gt; update the user -&gt; delete the user at last.</li> </ul> <pre><code>@flow(name=\"user_api_workflow_test\", log_prints=True, task_runner=SequentialTaskRunner())\ndef user_api_workflow_test():\n    print(\"Start test user API workflow \")\n    # Read test case configuration file\n    dict_test_case = utility.get_test_case_config_info(TEST_CASE_FILE_PATH)\n\n    # Go through each test case\n    for test_case_id in dict_test_case:\n        print(\"Task 1: Get Existing user\")\n        exist_user_id = dict_test_case[test_case_id][\"user_id\"]\n        dict_result = user_api_task.get_user_task(exist_user_id)\n        if dict_result[\"status_code\"] == 200:\n            print(\"Test Passed! This user info : \", dict_result[\"result\"])\n            case_result = \"Passed\"\n        else:\n            print(\"Test Failed! Failed to create a new user\")\n            case_result = \"Failed\"\n\n        TEST_RESULTS.append({'Test Case ID': test_case_id, 'Test Task Description': \"Task 1: Get Existing user\",\n                             'Test Result': case_result})\n        new_user_id = \"\"\n        print(\"Task 2: Create a new user\")\n        new_user_info = utility.get_json_data(dict_test_case[test_case_id][\"new_user_json_path\"])\n        dict_result = user_api_task.create_user_task(new_user_info)\n        if dict_result[\"status_code\"] == 201:\n            print(\"Test Passed! New User info : \", dict_result[\"result\"])\n            new_user_id = dict_result[\"result\"][\"user_id\"]\n            case_result = \"Passed\"\n        else:\n            print(\"Test Failed! Failed to create a new user\")\n            case_result = \"Failed\"\n\n        TEST_RESULTS.append({'Test Case ID': test_case_id, 'Test Task Description': \"Task 2: Create a new user\",\n                             'Test Result': case_result})\n\n        print(\"Task 3: Update existing user\")\n        update_user_info = utility.get_json_data(dict_test_case[test_case_id][\"update_user_json_path\"])\n        dict_result = user_api_task.update_user_task(new_user_id, update_user_info)\n        if dict_result[\"status_code\"] == 200:\n            print(\"Test Passed! Success update user info\")\n            case_result = \"Passed\"\n        else:\n            print(\"Test Failed! Failed to Update the user\")\n            case_result = \"Failed\"\n\n        TEST_RESULTS.append({'Test Case ID': test_case_id, 'Test Task Description': \"Task 3: Update existing user\",\n                             'Test Result': case_result})\n\n        print(\"Task 4: Delete existing user\")\n        dict_result = user_api_task.delete_user_task(new_user_id)\n        if dict_result[\"status_code\"] == 200:\n            print(f\"Test Passed! User {new_user_id} have been deleted\")\n            case_result = \"Passed\"\n        else:\n            print(\"Test Failed! Failed to delete the user\")\n            case_result = \"Failed\"\n\n        TEST_RESULTS.append({'Test Case ID': test_case_id, 'Test Task Description': \"Task 4: Delete existing user\",\n                             'Test Result': case_result})\n\n        utility.create_result_artifact_table(\"user api workflow\", test_case_id, TEST_RESULTS)\n\n    print(\"Finish test User API workflow\")\n\nif __name__ == \"__main__\":\n    user_api_workflow_test.serve(name=\"user_api_workflow\")\n</code></pre> <ul> <li>Let\u2019s see what would looks like in Prefect UI after executed this user API workflow:</li> </ul> <p>Just reminder, Both workflow and Tasks share some common features that\u2019s would be benefits us:</p> <ol> <li>Both are defined easily using their respective decorator, which accepts settings for that flow / task (see all task settings / flow settings).</li> <li>Each can be given a name, description and tags for organization and bookkeeping.</li> <li>provide functionality for retries, timeouts, and other hooks to handle failure and completion events.</li> </ol>"},{"location":"blog/2024/01/28/test-and-validate-data-pipelinesworkflows-with-prefect/#sample-about-task-runners-in-parallel","title":"Sample about Task Runners in parallel","text":"<p>Many real-world data workflows benefit from true parallel, distributed task execution. For these use cases, Prefect Integrations offers Prefect-developed task runners for parallel task execution as below:</p> <ul> <li>DaskTaskRunner: runs tasks requiring parallel execution using dask.distributed.</li> <li>RayTaskRunner: runs tasks requiring parallel execution using Ray.</li> </ul> <p>This provides us with the opportunity not only to conduct data workflow/pipeline testing but also to explore performance testing capabilities by executing certain tasks in parallel. Below is sample code for parallels workflow:</p> <pre><code>@task(name=\"square_calculator\", description=\"square calculator\", log_prints=True)\ndef square_calculator(input_data_1: int):\n    square = input_data_1 ** 2\n    print(square)\n\n\n@flow(name=\"workflow_with_parallel_tasks\", task_runner=DaskTaskRunner())\ndef workflow_with_parallel_tasks(data_list: list):\n    for data_item in data_list:\n        square_calculator.submit(data_item)\n\n\nif __name__ == \"__main__\":\n    input_list = [10, 15, 20, 25]\n    workflow_with_parallel_tasks(input_list)\n</code></pre> <ul> <li>Let\u2019s see parallel tasks processed in Prefect UI </li> </ul>"},{"location":"blog/2024/01/28/test-and-validate-data-pipelinesworkflows-with-prefect/#test-case-and-test-data-management","title":"Test Case and Test Data Management","text":"<p>For testing and data validation purposes, it\u2019s essential to have a flexible and straightforward path for maintaining test cases and test data. Prefect is a data orchestration tool, but it does not provide any build in solution for management test case and test data. I define my own approach by create YAML file to present test cases, include test data in JSON files.</p> <ul> <li>below is sample of user_workflow.yaml test case:</li> </ul> <pre><code>test-scenario-1:\n  user_id: \"1\"\n  new_user_json_path: \"../test_data/new_user_1.json\"\n  update_user_json_path: \"../test_data/update_user_1.json\"\ntest-scenario-2:\n  user_id: \"2\"\n  new_user_json_path: \"../test_data/new_user_2.json\"\n  update_user_json_path:  \"../test_data/update_user_2.json\"\n</code></pre> <ul> <li>below is sample of new_user_1.json test case: <pre><code>{\n  \"user_id\": \"1\",\n  \"name\": \"user_1\",\n  \"phone_number\": \"123-456-7890\"\n}\n</code></pre></li> </ul> <p>Think about this, by creating a common set of functions dedicated to reading test cases and extracting test data from JSON files.  The beauty of this &gt;approach lies in its simplicity \u2014 any new test case or test data can effortlessly be placed into a designated folder.  This enables the seamless addition of a multitude of data-driven test scenarios without the need to delve into the workflow or tasks code.</p>"},{"location":"blog/2024/01/28/test-and-validate-data-pipelinesworkflows-with-prefect/#deployment-cicd-and-schedule","title":"Deployment, CI/CD and Schedule","text":"<p>Prefect offers benefits and features on deployment strategy, deploying a flow exposes an API and UI so that you can:</p> <ul> <li>trigger new runs, cancel active runs, pause scheduled runs, customize parameters, and more.</li> <li>remotely configure schedules and automation rules for your deployments.</li> <li>dynamically provision infrastructure using workers.</li> </ul> <p></p> <p>Now you will be able to control or schedule when you execute tests, or easily implement test automation.</p>"},{"location":"blog/2024/01/28/test-and-validate-data-pipelinesworkflows-with-prefect/#extra-bonus-functionality-monitoring-logs-and-artifacts","title":"Extra Bonus Functionality: Monitoring, Logs and Artifacts","text":"<ul> <li>Prefect enables you to log a variety of useful information about your flow and task runs, capturing information about your workflows for purposes such as monitoring, troubleshooting, and auditing. This give us good information for testing step process.</li> <li>Prefect Artifacts which are formatted outputs rendered in Prefect UI, such as markdown, tables, or links. OK, now I decide to reused to track my test result summary for each time execution.</li> </ul> <p>I will put sample code on my GitHub, you could be able try this by yourself. It would include detail instructions on how to set up your own Prefect instance on your local machine. Please access the GitHub repository for details.</p>"},{"location":"blog/2024/01/28/test-and-validate-data-pipelinesworkflows-with-prefect/#summary-of-thoughts-after-this-experiment","title":"Summary of Thoughts after this experiment","text":"<ul> <li>It\u2019s pretty simple and quick for me to create many customized workflows to test all kinds of scenarios right way, take advantage of all benefits and features build in Prefect. That\u2019s big win for fast-paced development life cycle.</li> <li>The product I worked on only support define and trigger its data pipelines/workflows through the User Interface, It\u2019s maybe more practical to provide orchestration capability allow clients to automation their processes. Now this experiment could give us a starting point for this feature.</li> <li>I believe this experimental project would also be flexible enough to adapt to clients\u2019 environments for testing and validation with every upgrade or new production release.</li> </ul> <p>There would have more to explore with Prefect and other Open Source platform/tools, I would like to discuss with you further.</p> <p>Happy Coding and Happy Life!</p>"},{"location":"projects/","title":"My GitHub Projects","text":""},{"location":"projects/#learning-materials","title":"Learning Materials","text":"<ul> <li>Data Stack Learning: https://vickyguo0907.github.io/datastack-learning/</li> <li>Simple Ml Ops Pipeline: https://github.com/VickyGuo0907/mlops-pipeline/</li> <li>ML&amp;AI Learning: https://vickyguo0907.github.io/ML-AI-learning/</li> </ul>"},{"location":"projects/#projects-in-progress","title":"Projects in Progress:","text":"<ul> <li>Prefect Poc Porject: https://github.com/VickyGuo0907/prefect-poc-project</li> <li>mlops pipeline: https://github.com/VickyGuo0907/mlops-pipeline</li> </ul>"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2021/","title":"2021","text":""},{"location":"blog/archive/2020/","title":"2020","text":""},{"location":"blog/category/solution/","title":"Solution","text":""},{"location":"blog/category/commands/","title":"Commands","text":""}]}